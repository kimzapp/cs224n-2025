{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6070e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\cs224\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21252e38f90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16bfda",
   "metadata": {},
   "source": [
    "Raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ce0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba8339",
   "metadata": {},
   "source": [
    "Build vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4ad6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(raw_text.lower())\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbba95b",
   "metadata": {},
   "source": [
    "Build context and target pari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac14e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 random samples\n",
      "[(['we', 'are', 'to', 'study'], 'about'),\n",
      " (['are', 'about', 'study', 'the'], 'to'),\n",
      " (['about', 'to', 'the', 'idea'], 'study'),\n",
      " (['to', 'study', 'idea', 'of'], 'the'),\n",
      " (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "context_size = 2\n",
    "\n",
    "data = []\n",
    "for token in tokens[context_size:-context_size]:  # ignore boundary tokens\n",
    "    token_index = tokens.index(token)\n",
    "    context = tokens[token_index - context_size: token_index] + tokens[token_index + 1: token_index + context_size + 1]\n",
    "    target = token\n",
    "    data.append((context, target))\n",
    "\n",
    "print(\"5 random samples\")\n",
    "pprint(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b52aa",
   "metadata": {},
   "source": [
    "Helper function to create context vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c975f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37, 44, 19, 39])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ind2word = {i:w for i, w in enumerate(vocab)}\n",
    "word2ind = {w:i for i, w in enumerate(vocab)}\n",
    "\n",
    "def make_context_vector(context, word2ind):\n",
    "    idxs = [word2ind[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "make_context_vector(data[0][0], word2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838b7da",
   "metadata": {},
   "source": [
    "CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48dbb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.proj = nn.Linear(in_features=embedding_dim, out_features=128)\n",
    "        self.output = nn.Linear(in_features=128, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = (1 / (2 * context_size) * self.embedding(inputs).sum(dim=0)).view(1, -1)\n",
    "        proj = self.proj(embeds)\n",
    "        out = self.output(proj)\n",
    "        nll_prob = F.log_softmax(out, dim=-1)\n",
    "        return nll_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39497205",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0bfba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "cbow = CBOW(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "317ab497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 253.27525734901428\n",
      "Epoch 2, Loss: 251.52425003051758\n",
      "Epoch 3, Loss: 249.7962474822998\n",
      "Epoch 4, Loss: 248.09078311920166\n",
      "Epoch 5, Loss: 246.40746545791626\n",
      "Epoch 6, Loss: 244.7459716796875\n",
      "Epoch 7, Loss: 243.10603713989258\n",
      "Epoch 8, Loss: 241.48746848106384\n",
      "Epoch 9, Loss: 239.89012360572815\n",
      "Epoch 10, Loss: 238.3139045238495\n",
      "Epoch 11, Loss: 236.75876545906067\n",
      "Epoch 12, Loss: 235.224684715271\n",
      "Epoch 13, Loss: 233.7116768360138\n",
      "Epoch 14, Loss: 232.21977424621582\n",
      "Epoch 15, Loss: 230.74901366233826\n",
      "Epoch 16, Loss: 229.2994315624237\n",
      "Epoch 17, Loss: 227.87105751037598\n",
      "Epoch 18, Loss: 226.46389985084534\n",
      "Epoch 19, Loss: 225.0779379606247\n",
      "Epoch 20, Loss: 223.71311128139496\n",
      "Epoch 21, Loss: 222.36931705474854\n",
      "Epoch 22, Loss: 221.04640233516693\n",
      "Epoch 23, Loss: 219.74415981769562\n",
      "Epoch 24, Loss: 218.4623212814331\n",
      "Epoch 25, Loss: 217.2005672454834\n",
      "Epoch 26, Loss: 215.95851504802704\n",
      "Epoch 27, Loss: 214.73573327064514\n",
      "Epoch 28, Loss: 213.53174781799316\n",
      "Epoch 29, Loss: 212.34603118896484\n",
      "Epoch 30, Loss: 211.1780322790146\n",
      "Epoch 31, Loss: 210.0271645784378\n",
      "Epoch 32, Loss: 208.89282834529877\n",
      "Epoch 33, Loss: 207.77441024780273\n",
      "Epoch 34, Loss: 206.67129397392273\n",
      "Epoch 35, Loss: 205.58286714553833\n",
      "Epoch 36, Loss: 204.50852918624878\n",
      "Epoch 37, Loss: 203.44768857955933\n",
      "Epoch 38, Loss: 202.39977777004242\n",
      "Epoch 39, Loss: 201.36424750089645\n",
      "Epoch 40, Loss: 200.3405785560608\n",
      "Epoch 41, Loss: 199.32827335596085\n",
      "Epoch 42, Loss: 198.32686287164688\n",
      "Epoch 43, Loss: 197.33590000867844\n",
      "Epoch 44, Loss: 196.35497373342514\n",
      "Epoch 45, Loss: 195.3836945295334\n",
      "Epoch 46, Loss: 194.42169499397278\n",
      "Epoch 47, Loss: 193.46863651275635\n",
      "Epoch 48, Loss: 192.52419900894165\n",
      "Epoch 49, Loss: 191.5880817770958\n",
      "Epoch 50, Loss: 190.66000586748123\n",
      "Epoch 51, Loss: 189.73971444368362\n",
      "Epoch 52, Loss: 188.82695788145065\n",
      "Epoch 53, Loss: 187.9215098619461\n",
      "Epoch 54, Loss: 187.02315145730972\n",
      "Epoch 55, Loss: 186.13167649507523\n",
      "Epoch 56, Loss: 185.24689561128616\n",
      "Epoch 57, Loss: 184.36862587928772\n",
      "Epoch 58, Loss: 183.4966950416565\n",
      "Epoch 59, Loss: 182.63093495368958\n",
      "Epoch 60, Loss: 181.77119308710098\n",
      "Epoch 61, Loss: 180.91732043027878\n",
      "Epoch 62, Loss: 180.06917786598206\n",
      "Epoch 63, Loss: 179.22662276029587\n",
      "Epoch 64, Loss: 178.3895303606987\n",
      "Epoch 65, Loss: 177.557778775692\n",
      "Epoch 66, Loss: 176.73124277591705\n",
      "Epoch 67, Loss: 175.90981036424637\n",
      "Epoch 68, Loss: 175.09337258338928\n",
      "Epoch 69, Loss: 174.28182208538055\n",
      "Epoch 70, Loss: 173.47505968809128\n",
      "Epoch 71, Loss: 172.67299151420593\n",
      "Epoch 72, Loss: 171.87551909685135\n",
      "Epoch 73, Loss: 171.08254730701447\n",
      "Epoch 74, Loss: 170.29400372505188\n",
      "Epoch 75, Loss: 169.50979489088058\n",
      "Epoch 76, Loss: 168.7298453450203\n",
      "Epoch 77, Loss: 167.9540719985962\n",
      "Epoch 78, Loss: 167.1824068427086\n",
      "Epoch 79, Loss: 166.41477689146996\n",
      "Epoch 80, Loss: 165.6511059999466\n",
      "Epoch 81, Loss: 164.89134085178375\n",
      "Epoch 82, Loss: 164.13540849089622\n",
      "Epoch 83, Loss: 163.38325560092926\n",
      "Epoch 84, Loss: 162.6348150074482\n",
      "Epoch 85, Loss: 161.89003363251686\n",
      "Epoch 86, Loss: 161.1488590836525\n",
      "Epoch 87, Loss: 160.41123977303505\n",
      "Epoch 88, Loss: 159.67711716890335\n",
      "Epoch 89, Loss: 158.94645074009895\n",
      "Epoch 90, Loss: 158.21919104456902\n",
      "Epoch 91, Loss: 157.4952918291092\n",
      "Epoch 92, Loss: 156.77470940351486\n",
      "Epoch 93, Loss: 156.05740824341774\n",
      "Epoch 94, Loss: 155.34333708882332\n",
      "Epoch 95, Loss: 154.6324618458748\n",
      "Epoch 96, Loss: 153.9247469007969\n",
      "Epoch 97, Loss: 153.22015750408173\n",
      "Epoch 98, Loss: 152.51865169405937\n",
      "Epoch 99, Loss: 151.82020071148872\n",
      "Epoch 100, Loss: 151.12477380037308\n",
      "Epoch 101, Loss: 150.43232995271683\n",
      "Epoch 102, Loss: 149.74285116791725\n",
      "Epoch 103, Loss: 149.05629581212997\n",
      "Epoch 104, Loss: 148.37264168262482\n",
      "Epoch 105, Loss: 147.6918587386608\n",
      "Epoch 106, Loss: 147.01391595602036\n",
      "Epoch 107, Loss: 146.33879977464676\n",
      "Epoch 108, Loss: 145.66647306084633\n",
      "Epoch 109, Loss: 144.99691534042358\n",
      "Epoch 110, Loss: 144.33010616898537\n",
      "Epoch 111, Loss: 143.66601315140724\n",
      "Epoch 112, Loss: 143.00462275743484\n",
      "Epoch 113, Loss: 142.34590736031532\n",
      "Epoch 114, Loss: 141.68985080718994\n",
      "Epoch 115, Loss: 141.03642931580544\n",
      "Epoch 116, Loss: 140.3856222331524\n",
      "Epoch 117, Loss: 139.737408131361\n",
      "Epoch 118, Loss: 139.09177401661873\n",
      "Epoch 119, Loss: 138.44869288802147\n",
      "Epoch 120, Loss: 137.8081591129303\n",
      "Epoch 121, Loss: 137.1701452434063\n",
      "Epoch 122, Loss: 136.5346359014511\n",
      "Epoch 123, Loss: 135.9016170501709\n",
      "Epoch 124, Loss: 135.2710730433464\n",
      "Epoch 125, Loss: 134.64298623800278\n",
      "Epoch 126, Loss: 134.01734152436256\n",
      "Epoch 127, Loss: 133.3941248357296\n",
      "Epoch 128, Loss: 132.77331972122192\n",
      "Epoch 129, Loss: 132.15491825342178\n",
      "Epoch 130, Loss: 131.53889992833138\n",
      "Epoch 131, Loss: 130.92525485157967\n",
      "Epoch 132, Loss: 130.31396788358688\n",
      "Epoch 133, Loss: 129.70503211021423\n",
      "Epoch 134, Loss: 129.09843188524246\n",
      "Epoch 135, Loss: 128.49415609240532\n",
      "Epoch 136, Loss: 127.89219433069229\n",
      "Epoch 137, Loss: 127.29253038764\n",
      "Epoch 138, Loss: 126.69516056776047\n",
      "Epoch 139, Loss: 126.10006719827652\n",
      "Epoch 140, Loss: 125.50724521279335\n",
      "Epoch 141, Loss: 124.91668555140495\n",
      "Epoch 142, Loss: 124.32837441563606\n",
      "Epoch 143, Loss: 123.74231168627739\n",
      "Epoch 144, Loss: 123.15847474336624\n",
      "Epoch 145, Loss: 122.57686853408813\n",
      "Epoch 146, Loss: 121.99747478961945\n",
      "Epoch 147, Loss: 121.42028921842575\n",
      "Epoch 148, Loss: 120.84530378878117\n",
      "Epoch 149, Loss: 120.27251121401787\n",
      "Epoch 150, Loss: 119.70190232992172\n",
      "Epoch 151, Loss: 119.13347315788269\n",
      "Epoch 152, Loss: 118.5672150105238\n",
      "Epoch 153, Loss: 118.00312031805515\n",
      "Epoch 154, Loss: 117.44117991626263\n",
      "Epoch 155, Loss: 116.8813923150301\n",
      "Epoch 156, Loss: 116.32374858856201\n",
      "Epoch 157, Loss: 115.76824353635311\n",
      "Epoch 158, Loss: 115.21487456560135\n",
      "Epoch 159, Loss: 114.66363123059273\n",
      "Epoch 160, Loss: 114.1145084053278\n",
      "Epoch 161, Loss: 113.56750538945198\n",
      "Epoch 162, Loss: 113.02261072397232\n",
      "Epoch 163, Loss: 112.47982463240623\n",
      "Epoch 164, Loss: 111.93914072215557\n",
      "Epoch 165, Loss: 111.40055058896542\n",
      "Epoch 166, Loss: 110.86405692994595\n",
      "Epoch 167, Loss: 110.32964362204075\n",
      "Epoch 168, Loss: 109.79731920361519\n",
      "Epoch 169, Loss: 109.26707264780998\n",
      "Epoch 170, Loss: 108.73890209197998\n",
      "Epoch 171, Loss: 108.21280153095722\n",
      "Epoch 172, Loss: 107.6887663602829\n",
      "Epoch 173, Loss: 107.16679464280605\n",
      "Epoch 174, Loss: 106.64688037335873\n",
      "Epoch 175, Loss: 106.12902437150478\n",
      "Epoch 176, Loss: 105.61321876943111\n",
      "Epoch 177, Loss: 105.09946213662624\n",
      "Epoch 178, Loss: 104.58775267004967\n",
      "Epoch 179, Loss: 104.0780790001154\n",
      "Epoch 180, Loss: 103.57044366002083\n",
      "Epoch 181, Loss: 103.06484638154507\n",
      "Epoch 182, Loss: 102.56127986311913\n",
      "Epoch 183, Loss: 102.05973890423775\n",
      "Epoch 184, Loss: 101.56022401154041\n",
      "Epoch 185, Loss: 101.06273321807384\n",
      "Epoch 186, Loss: 100.5672616660595\n",
      "Epoch 187, Loss: 100.07380501925945\n",
      "Epoch 188, Loss: 99.58235874772072\n",
      "Epoch 189, Loss: 99.09291967749596\n",
      "Epoch 190, Loss: 98.60548737645149\n",
      "Epoch 191, Loss: 98.1200610846281\n",
      "Epoch 192, Loss: 97.63663209974766\n",
      "Epoch 193, Loss: 97.15520054101944\n",
      "Epoch 194, Loss: 96.67576216161251\n",
      "Epoch 195, Loss: 96.19831570982933\n",
      "Epoch 196, Loss: 95.7228571176529\n",
      "Epoch 197, Loss: 95.24938480556011\n",
      "Epoch 198, Loss: 94.77789551019669\n",
      "Epoch 199, Loss: 94.3083824813366\n",
      "Epoch 200, Loss: 93.84084743261337\n",
      "Epoch 201, Loss: 93.37528218328953\n",
      "Epoch 202, Loss: 92.91168753802776\n",
      "Epoch 203, Loss: 92.45005887746811\n",
      "Epoch 204, Loss: 91.99039486050606\n",
      "Epoch 205, Loss: 91.53269216418266\n",
      "Epoch 206, Loss: 91.07694435119629\n",
      "Epoch 207, Loss: 90.62314277887344\n",
      "Epoch 208, Loss: 90.17130061984062\n",
      "Epoch 209, Loss: 89.72140185534954\n",
      "Epoch 210, Loss: 89.27345027029514\n",
      "Epoch 211, Loss: 88.82743361592293\n",
      "Epoch 212, Loss: 88.38335309922695\n",
      "Epoch 213, Loss: 87.94120702147484\n",
      "Epoch 214, Loss: 87.50098897516727\n",
      "Epoch 215, Loss: 87.0626998692751\n",
      "Epoch 216, Loss: 86.62633541226387\n",
      "Epoch 217, Loss: 86.19188977777958\n",
      "Epoch 218, Loss: 85.75935910642147\n",
      "Epoch 219, Loss: 85.32874019443989\n",
      "Epoch 220, Loss: 84.90003025531769\n",
      "Epoch 221, Loss: 84.47322280704975\n",
      "Epoch 222, Loss: 84.04831351339817\n",
      "Epoch 223, Loss: 83.62530213594437\n",
      "Epoch 224, Loss: 83.20419047772884\n",
      "Epoch 225, Loss: 82.78496424853802\n",
      "Epoch 226, Loss: 82.367619946599\n",
      "Epoch 227, Loss: 81.95215837657452\n",
      "Epoch 228, Loss: 81.53857511281967\n",
      "Epoch 229, Loss: 81.12686455249786\n",
      "Epoch 230, Loss: 80.71702070534229\n",
      "Epoch 231, Loss: 80.30903927981853\n",
      "Epoch 232, Loss: 79.90292076766491\n",
      "Epoch 233, Loss: 79.49866017699242\n",
      "Epoch 234, Loss: 79.09625327587128\n",
      "Epoch 235, Loss: 78.695685736835\n",
      "Epoch 236, Loss: 78.29696584492922\n",
      "Epoch 237, Loss: 77.9000840485096\n",
      "Epoch 238, Loss: 77.50503766536713\n",
      "Epoch 239, Loss: 77.11182454228401\n",
      "Epoch 240, Loss: 76.72043569386005\n",
      "Epoch 241, Loss: 76.33086308091879\n",
      "Epoch 242, Loss: 75.94311044365168\n",
      "Epoch 243, Loss: 75.55717078596354\n",
      "Epoch 244, Loss: 75.17303939908743\n",
      "Epoch 245, Loss: 74.79071360826492\n",
      "Epoch 246, Loss: 74.41017970442772\n",
      "Epoch 247, Loss: 74.03144104033709\n",
      "Epoch 248, Loss: 73.6544923260808\n",
      "Epoch 249, Loss: 73.27932653576136\n",
      "Epoch 250, Loss: 72.90594226121902\n",
      "Epoch 251, Loss: 72.5343320146203\n",
      "Epoch 252, Loss: 72.16449630260468\n",
      "Epoch 253, Loss: 71.7964219674468\n",
      "Epoch 254, Loss: 71.43010504543781\n",
      "Epoch 255, Loss: 71.06554792076349\n",
      "Epoch 256, Loss: 70.70273993164301\n",
      "Epoch 257, Loss: 70.34168028086424\n",
      "Epoch 258, Loss: 69.98236235231161\n",
      "Epoch 259, Loss: 69.62478225678205\n",
      "Epoch 260, Loss: 69.26893004029989\n",
      "Epoch 261, Loss: 68.91480762511492\n",
      "Epoch 262, Loss: 68.56240659952164\n",
      "Epoch 263, Loss: 68.21172375231981\n",
      "Epoch 264, Loss: 67.86275333166122\n",
      "Epoch 265, Loss: 67.51548504829407\n",
      "Epoch 266, Loss: 67.16992555558681\n",
      "Epoch 267, Loss: 66.82605594396591\n",
      "Epoch 268, Loss: 66.48388490080833\n",
      "Epoch 269, Loss: 66.14340019971132\n",
      "Epoch 270, Loss: 65.8045963793993\n",
      "Epoch 271, Loss: 65.46746648848057\n",
      "Epoch 272, Loss: 65.13201868534088\n",
      "Epoch 273, Loss: 64.7982337474823\n",
      "Epoch 274, Loss: 64.46610843390226\n",
      "Epoch 275, Loss: 64.13564685732126\n",
      "Epoch 276, Loss: 63.80683347582817\n",
      "Epoch 277, Loss: 63.47966808080673\n",
      "Epoch 278, Loss: 63.15414455533028\n",
      "Epoch 279, Loss: 62.830264896154404\n",
      "Epoch 280, Loss: 62.50801368057728\n",
      "Epoch 281, Loss: 62.187387362122536\n",
      "Epoch 282, Loss: 61.86839083582163\n",
      "Epoch 283, Loss: 61.5510101467371\n",
      "Epoch 284, Loss: 61.23523795604706\n",
      "Epoch 285, Loss: 60.921073757112026\n",
      "Epoch 286, Loss: 60.60851386189461\n",
      "Epoch 287, Loss: 60.29755329340696\n",
      "Epoch 288, Loss: 59.988180451095104\n",
      "Epoch 289, Loss: 59.6803954616189\n",
      "Epoch 290, Loss: 59.37419889867306\n",
      "Epoch 291, Loss: 59.069578133523464\n",
      "Epoch 292, Loss: 58.7665261849761\n",
      "Epoch 293, Loss: 58.465040899813175\n",
      "Epoch 294, Loss: 58.16511972248554\n",
      "Epoch 295, Loss: 57.866755835711956\n",
      "Epoch 296, Loss: 57.569948613643646\n",
      "Epoch 297, Loss: 57.27467967569828\n",
      "Epoch 298, Loss: 56.980957582592964\n",
      "Epoch 299, Loss: 56.68876986950636\n",
      "Epoch 300, Loss: 56.39811248332262\n",
      "Epoch 301, Loss: 56.10898180305958\n",
      "Epoch 302, Loss: 55.821374870836735\n",
      "Epoch 303, Loss: 55.53528542816639\n",
      "Epoch 304, Loss: 55.250702656805515\n",
      "Epoch 305, Loss: 54.96762715280056\n",
      "Epoch 306, Loss: 54.68605510890484\n",
      "Epoch 307, Loss: 54.40597828477621\n",
      "Epoch 308, Loss: 54.12738875299692\n",
      "Epoch 309, Loss: 53.850286804139614\n",
      "Epoch 310, Loss: 53.57466417551041\n",
      "Epoch 311, Loss: 53.300520062446594\n",
      "Epoch 312, Loss: 53.02784322202206\n",
      "Epoch 313, Loss: 52.756630048155785\n",
      "Epoch 314, Loss: 52.48687871545553\n",
      "Epoch 315, Loss: 52.218580201268196\n",
      "Epoch 316, Loss: 51.95173303037882\n",
      "Epoch 317, Loss: 51.68632734566927\n",
      "Epoch 318, Loss: 51.422358490526676\n",
      "Epoch 319, Loss: 51.15982523560524\n",
      "Epoch 320, Loss: 50.89871986955404\n",
      "Epoch 321, Loss: 50.63903719931841\n",
      "Epoch 322, Loss: 50.38077377527952\n",
      "Epoch 323, Loss: 50.12392254173756\n",
      "Epoch 324, Loss: 49.86847874522209\n",
      "Epoch 325, Loss: 49.614435724914074\n",
      "Epoch 326, Loss: 49.361789017915726\n",
      "Epoch 327, Loss: 49.11053277552128\n",
      "Epoch 328, Loss: 48.860664278268814\n",
      "Epoch 329, Loss: 48.612177304923534\n",
      "Epoch 330, Loss: 48.36506297439337\n",
      "Epoch 331, Loss: 48.11932072043419\n",
      "Epoch 332, Loss: 47.874939769506454\n",
      "Epoch 333, Loss: 47.63192168623209\n",
      "Epoch 334, Loss: 47.39025441557169\n",
      "Epoch 335, Loss: 47.14993938058615\n",
      "Epoch 336, Loss: 46.91096343100071\n",
      "Epoch 337, Loss: 46.673325791954994\n",
      "Epoch 338, Loss: 46.4370251968503\n",
      "Epoch 339, Loss: 46.20205193012953\n",
      "Epoch 340, Loss: 45.96839629113674\n",
      "Epoch 341, Loss: 45.736056957393885\n",
      "Epoch 342, Loss: 45.50502871349454\n",
      "Epoch 343, Loss: 45.27530957758427\n",
      "Epoch 344, Loss: 45.04688508063555\n",
      "Epoch 345, Loss: 44.81975944712758\n",
      "Epoch 346, Loss: 44.593921445310116\n",
      "Epoch 347, Loss: 44.36936563625932\n",
      "Epoch 348, Loss: 44.14608809351921\n",
      "Epoch 349, Loss: 43.92408800125122\n",
      "Epoch 350, Loss: 43.703352369368076\n",
      "Epoch 351, Loss: 43.48387833684683\n",
      "Epoch 352, Loss: 43.26565671339631\n",
      "Epoch 353, Loss: 43.04868585243821\n",
      "Epoch 354, Loss: 42.83296101912856\n",
      "Epoch 355, Loss: 42.61847988143563\n",
      "Epoch 356, Loss: 42.40523240715265\n",
      "Epoch 357, Loss: 42.19321712851524\n",
      "Epoch 358, Loss: 41.982418429106474\n",
      "Epoch 359, Loss: 41.7728433907032\n",
      "Epoch 360, Loss: 41.56447568535805\n",
      "Epoch 361, Loss: 41.357316825538874\n",
      "Epoch 362, Loss: 41.15136097744107\n",
      "Epoch 363, Loss: 40.94659870862961\n",
      "Epoch 364, Loss: 40.74302364140749\n",
      "Epoch 365, Loss: 40.540637511759996\n",
      "Epoch 366, Loss: 40.33943250775337\n",
      "Epoch 367, Loss: 40.13939763978124\n",
      "Epoch 368, Loss: 39.94052963703871\n",
      "Epoch 369, Loss: 39.74282390251756\n",
      "Epoch 370, Loss: 39.546283937990665\n",
      "Epoch 371, Loss: 39.35088988766074\n",
      "Epoch 372, Loss: 39.156640477478504\n",
      "Epoch 373, Loss: 38.96353041008115\n",
      "Epoch 374, Loss: 38.77155788242817\n",
      "Epoch 375, Loss: 38.58071472495794\n",
      "Epoch 376, Loss: 38.39099533483386\n",
      "Epoch 377, Loss: 38.20239282026887\n",
      "Epoch 378, Loss: 38.01490702480078\n",
      "Epoch 379, Loss: 37.82852639630437\n",
      "Epoch 380, Loss: 37.64324666187167\n",
      "Epoch 381, Loss: 37.459067694842815\n",
      "Epoch 382, Loss: 37.27597761899233\n",
      "Epoch 383, Loss: 37.093970220535994\n",
      "Epoch 384, Loss: 36.91304516419768\n",
      "Epoch 385, Loss: 36.73319570720196\n",
      "Epoch 386, Loss: 36.55441313609481\n",
      "Epoch 387, Loss: 36.37669511139393\n",
      "Epoch 388, Loss: 36.20003293082118\n",
      "Epoch 389, Loss: 36.02442502975464\n",
      "Epoch 390, Loss: 35.84986322000623\n",
      "Epoch 391, Loss: 35.67634071409702\n",
      "Epoch 392, Loss: 35.503857277333736\n",
      "Epoch 393, Loss: 35.33240244910121\n",
      "Epoch 394, Loss: 35.16197366267443\n",
      "Epoch 395, Loss: 34.99256154894829\n",
      "Epoch 396, Loss: 34.82416893169284\n",
      "Epoch 397, Loss: 34.65678342059255\n",
      "Epoch 398, Loss: 34.490399193018675\n",
      "Epoch 399, Loss: 34.3250146061182\n",
      "Epoch 400, Loss: 34.16062135621905\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cbow.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word2ind).to(device)\n",
    "        target_vector = torch.tensor([word2ind[target]], dtype=torch.long).to(device)\n",
    "\n",
    "        nll_prob = cbow(context_vector)\n",
    "\n",
    "        cbow.zero_grad()\n",
    "\n",
    "        l = loss(nll_prob, target_vector)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += l.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd049c",
   "metadata": {},
   "source": [
    "Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fddc56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: We are about to study the idea of a computational process.\n",
      "Computational processes are abstract beings that inhabit computers.\n",
      "As they evolve, processes manipulate other abstract things called data.\n",
      "The evolution of a process is directed by a pattern of rules\n",
      "called a program. People create programs to direct processes. In effect,\n",
      "we conjure the spirits of the computer with our spells.\n",
      "Test Context: ['we', 'are', 'to', 'study']\n",
      "\n",
      "Prediction: about\n"
     ]
    }
   ],
   "source": [
    "context = ['we','are','to', 'study']\n",
    "context_vector = make_context_vector(context, word2ind).to(device)\n",
    "a = cbow(context_vector).detach().cpu().squeeze()\n",
    "print(f'Raw text: {raw_text}')\n",
    "print(f'Test Context: {context}')\n",
    "max_idx = torch.argmax(a, axis=-1).item()\n",
    "print(f'\\nPrediction: {ind2word[max_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7720634",
   "metadata": {},
   "source": [
    "Visualize embedding results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a8fe080",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = cbow.embedding\n",
    "sample_words = ['process', 'computer', 'data', 'program', 'people', 'we']\n",
    "embedded_words = {word: embedder.weight[word2ind[word]].detach().cpu().numpy() for word in sample_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdc89487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'process': array([ 0.64501125, -0.4071673 ,  0.5174458 , -0.05592355, -0.14806238,\n",
       "         1.6818755 , -0.35157594,  1.1424899 , -1.4515704 ,  0.22997485],\n",
       "       dtype=float32),\n",
       " 'computer': array([-1.6185249 ,  1.3097272 , -1.6790228 ,  0.40622348, -0.8195215 ,\n",
       "        -1.286203  , -0.9047828 ,  0.24237242, -0.10927751, -0.59898895],\n",
       "       dtype=float32),\n",
       " 'data': array([-0.39453563, -1.306132  ,  1.5013225 , -0.78736126,  2.1178265 ,\n",
       "        -0.7634887 ,  1.0542344 ,  1.649025  ,  0.38630095, -0.5839078 ],\n",
       "       dtype=float32),\n",
       " 'program': array([-2.4732754 ,  0.51105267, -0.373866  ,  0.48467186, -0.17955738,\n",
       "         0.21877593, -0.8221533 , -0.8864248 ,  0.5289834 , -0.4929878 ],\n",
       "       dtype=float32),\n",
       " 'people': array([-0.2662421 ,  0.42444646,  1.2033688 , -1.4567262 ,  1.0492752 ,\n",
       "         0.15062292,  0.24350879, -1.2094615 ,  0.93903244, -0.32820055],\n",
       "       dtype=float32),\n",
       " 'we': array([ 0.33883968, -2.3187532 ,  0.19369777,  0.783307  , -0.9726918 ,\n",
       "        -0.486766  , -0.600468  ,  1.184485  , -0.99740535, -0.15826018],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "006d22b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAH5CAYAAABDK7L8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAN9VJREFUeJzt3Xl8VPW9//H3JCGTfchCEpYAKQiCKCgikoIkhF2QxYqAqKEqRYGflFu1oCWBWw1VrxW1oLYKqGGxKohQLSAB9CoCCiogWDaDZgECTEKAQJLz+wOdy5gEyFcmk+X1fDzmcTvf+eaczyR99L48nkxslmVZAgAAAFBlPt4eAAAAAKitiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIT9vD3AhZWVlys7OVmhoqGw2m7fHAQAAwM9YlqXCwkI1adJEPj717zptjY7p7OxsxcXFeXsMAAAAXMTBgwfVrFkzb49R7Wp0TIeGhko698MJCwvz8jQAAAD4uYKCAsXFxbm6rb6p0TH9060dYWFhxDQAAEANVl9vya1/N7YAAAAAlwkxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgC8LDExUYmJid4eAwBggJgGAAAADBHTAAAAgCFiGkCdlpaWJpvNpq1bt2r48OEKCwuTw+HQmDFjdPjwYbe9S5YsUbdu3RQcHKyQkBD169dPW7duLXfM5cuXq1u3bgoKClJoaKj69OmjTz/91Pi8FTlz5oz+/Oc/68orr5TdblejRo00duzYS/paAED1IaYB1AvDhg1T69at9dZbbyktLU3Lli1Tv379dPbsWUnSE088oVGjRql9+/Z688039frrr6uwsFA9evTQzp07XcdZuHChhgwZorCwMC1atEivvPKKjh07psTERH388cdVPm9FysrKNGTIEM2aNUujR4/WypUrNWvWLK1evVqJiYk6derU5f8GAQDMWDWY0+m0JFlOp9PbowCopVJTUy1J1u9//3u39YyMDEuS9cYbb1hZWVmWn5+fNWnSJLc9hYWFVmxsrDVixAjLsiyrtLTUatKkiXX11VdbpaWlbvuio6OthISEKp33Jz179rR69uzper5o0SJLkvX222+7fe3mzZstSdacOXPMvhkA4AH1vde4Mg2gXrjjjjvcno8YMUJ+fn7KzMzUv//9b5WUlOiuu+5SSUmJ6xEQEKCePXtq3bp1kqTdu3crOztbd955p3x8/u9/PkNCQnTrrbdq48aNOnny5CWftzIrVqxQw4YNNXjwYLd5OnXqpNjYWNc8AADv8/P2AABwuZSVWcr5z3EVFRQrOMyuxlc0dL0WGxvrttfPz0+RkZHKz89XXl6eJKlLly4VHvencM7Pz5ckNW7cuNyeJk2aqKysTMeOHVNQUNAlnbcyeXl5On78uPz9/St8/ciRI5V+LQCgehHTAOqEvVsP6aMl/1HR8WLXWnBDu44eK5Ik5ebmqmnTpq7XSkpKlJ+fr8jISEVFRUmS3nrrLbVo0aLSc0RGRkqScnJyyr2WnZ0tHx8fhYeHu61f6LyViYqKUmRkpD744IMKXw8NDa30awEA1YuYBlDr7d16SB+8tL3cetHxYu35/JAkKSMjQ507d3a99uabb6qkpESJiYnq3r27/Pz8tHfvXt16662Vnqdt27Zq2rSpFi5cqD/84Q+y2WznzlNUpLffftv1CR/nu9B5KzNo0CAtXrxYpaWl6tq16yV9DwAA3kFMA6jVysosfbTkPxfd984778jPz099+vTRjh079Kc//UkdO3bUiBEj5O/vr5kzZ+rRRx/Vvn371L9/f4WHhysvL0+bNm1ScHCwZsyYIR8fHz355JO64447NGjQIP3ud79TcXGxnnrqKR0/flyzZs2q0nkrM3LkSGVkZGjgwIF68MEHdcMNN6hBgwb6/vvvlZmZqSFDhmjYsGG/6PsGALg8iGkAtVrOf4673dpRmRefma8585/R3LlzZbPZNHjwYD377LOu+5KnTp2q9u3ba/bs2Vq0aJGKi4sVGxurLl26aPz48a7jjB49WsHBwUpPT9ftt98uX19f3XjjjcrMzFRCQkK5877zzjtKS0ur9LwV8fX11fLlyzV79my9/vrrSk9Pl5+fn5o1a6aePXvq6quvNvhOAQA8wWZZluXtISpTUFAgh8Mhp9OpsLAwb48DoAb6dnOuVr+ys9LXV25ZoPc/f02frtquG/tcVW1zpaWlacaMGTp8+LDrnmwAqIvqe6/x0XgAarXgMPul7Qu9tH0AAFQFMQ2gVmt8RUMFN7x4KMf8ylEN0wAA6htu8wBQ61X2aR4/6f+7Dmp1bXQ1TgQA9Ud97zWuTAOo9VpdG63+v+tQ7gp1SLidkAYAeBSf5gGgTmh1bbTiOzYq9xcQfXxs3h4NAFCHEdMA6gwfH5uatg2/+EYAAC4TbvMAAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEMejen09HR16dJFoaGhio6O1tChQ7V7925PnhIAAACoNh6N6fXr12vChAnauHGjVq9erZKSEvXt21dFRUWePC0AAABQLWyWZVnVdbLDhw8rOjpa69ev10033XTR/QUFBXI4HHI6nQoLC6uGCQEAAFAV9b3X/KrzZE6nU5IUERFR4evFxcUqLi52PS8oKKiWuQAAAAAT1fYLiJZlacqUKerevbs6dOhQ4Z709HQ5HA7XIy4urrrGAwAAAKqs2m7zmDBhglauXKmPP/5YzZo1q3BPRVem4+Li6u2/NgAAAKjpuM2jGkyaNEnLly/Xhg0bKg1pSbLb7bLb7dUxEgAAAPCLeTSmLcvSpEmTtHTpUq1bt07x8fGePB0AAABQrTwa0xMmTNDChQv17rvvKjQ0VLm5uZIkh8OhwMBAT54aAAAA8DiP3jNts9kqXJ83b55SUlIu+vX1/R4cAACAmq6+95rHb/MAAAAA6qpq+2g8AAAAoK4hpgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpA2lpabLZbFX+uoULF+rZZ5+9/AMBAADAK4jpakRMAwAA1C3ENAAAAGCImL6IlStXqlOnTrLb7YqPj9fTTz9dbs/f/vY33XTTTYqOjlZwcLCuvvpqPfnkkzp79qxrT2JiolauXKnvvvtONpvN9fjJjBkz1LVrV0VERCgsLEzXXXedXnnlFVmWVS3vEwAAAFXn5+0BarIPP/xQQ4YMUbdu3bR48WKVlpbqySefVF5entu+vXv3avTo0YqPj5e/v7++/PJLPf7449q1a5deffVVSdKcOXM0btw47d27V0uXLi13rgMHDuh3v/udmjdvLknauHGjJk2apB9++EHTp0/3/JsFAABAldmsGnzps6CgQA6HQ06nU2FhYdV+/htvvFEHDx7U3r17FRAQIEkqLCxUy5YtdfTo0QqvGpeVlamsrEyLFi3S2LFjdfjwYYWHh0uSBg0apO3bt+vAgQMXPO9Px0hPT9fs2bN1+PBho194BAAA8DRv95q3cZtHJYqKirR582YNHz7cFdKSFBoaqsGDB7vt3bp1q2655RZFRkbK19dXDRo00F133aXS0lJ9++23l3S+tWvXqnfv3nI4HK5jTJ8+Xfn5+Tp06NBlfW8AAAC4PLjN40elZaX64tAXOnzysBoFNVL0mWiVlZUpNja23N7z17KystSjRw+1bdtWs2fPVsuWLRUQEKBNmzZpwoQJOnXq1EXPvWnTJvXt21eJiYn6+9//rmbNmsnf31/Lli3T448/fknHAAAAQPUjpiWt+W6NZm2apbyT/3cvdJRPlGw2m3Jzc8vtP39t2bJlKioq0jvvvKMWLVq41rdt23bJ51+8eLEaNGigFStWuF0FX7ZsWdXeCAAAAKpVvb/NY813azRl3RS3kJak/LJ8BcQHaNE/F+n06dOu9cLCQr333nuu5z/dy2y3211rlmXp73//e7lz2e32Cq8y22w2+fn5ydfX17V26tQpvf766+ZvDAAAAB5Xr2O6tKxUszbNkqXyv0hoyVLM8BjlH8pX7z69tWzZMr399ttKTk5WcHCwa1+fPn3k7++vUaNG6f3339fSpUvVr18/HTt2rNwxr776ah06dEhz587Vpk2btGXLFknSzTffrBMnTmj06NFavXq1Fi9erB49ergFOgAAAGqeev1pHptzN+u3//7thWfYWiD/1f7K2pOl2NhYPfDAAzp16pRmzJjh+jSPFStW6LHHHtPu3bsVGRmp0aNHq1evXhowYIAyMzOVmJgoSTp27JjGjRunNWvWyOl0yrIs1zHmzZunv/zlLzpw4ICaNm2q++67T9HR0brnnnu0f/9+tWzZ8rK/fwAAgF+qvn+aR72O6X/t+5ce+eiRi+77S4+/aOCvBl728wMAANR29T2m6/VtHo2CGl3WfQAAAKhf6nVMXxd9nWKCYmRTxX8QxSabYoNidV30ddU8GQAAAGqDeh3Tvj6++uMNf5SkckH90/NHbnhEvj6+5b4WAAAAqNcxLUm9W/TWM4nPKDoo2m09JihGzyQ+o94tentpMgAAANR0/NEWnQvqpLgkt7+AeF30dVyRBgAAwAUR0z/y9fFVl9gu3h4DAAAAtUi9v80DAAAAMEVMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAw5NGY3rBhgwYPHqwmTZrIZrNp2bJlnjwdAAAAUK08GtNFRUXq2LGjXnjhBU+eBgAAAPAKP08efMCAARowYMAl7y8uLlZxcbHreUFBgSfGAgAAAC6LGnXPdHp6uhwOh+sRFxfn7ZEAAACAStWomJ46daqcTqfrcfDgQW+PBAAAAFTKo7d5VJXdbpfdbvf2GAAAAMAlqVFXpgEAAIDahJgGAAAADHn0No8TJ05oz549ruf79+/Xtm3bFBERoebNm3vy1AAAAIDHeTSmt2zZoqSkJNfzKVOmSJLuvvtuzZ8/35OnBgAAADzOozGdmJgoy7I8eQoAAADAa7hnGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAD4hU6dOlVt5zp58mS1nQsXR0wDAABISktLk81m09atWzV8+HCFhYXJ4XBozJgxOnz4sGtfy5YtNWjQIL3zzju69tprFRAQoBkzZkiStm/friFDhig8PFwBAQHq1KmTFixYUO5cO3bsUN++fRUUFKRGjRppwoQJWrlypWw2m9atW+fal5iYqA4dOmjDhg1KSEhQUFCQfvvb30qSlixZor59+6px48YKDAxUu3bt9Mc//lFFRUVu50pJSVFISIh27dqlfv36KTg4WI0bN9asWbMkSRs3blT37t0VHBysNm3aVDgvKufn7QEAAABqkmHDhmnEiBEaP368duzYoT/96U/auXOnPvvsMzVo0ECS9MUXX+ibb77RY489pvj4eAUHB2v37t1KSEhQdHS0nnvuOUVGRuqNN95QSkqK8vLy9PDDD0uScnJy1LNnTwUHB2vu3LmKjo7WokWLNHHixArnycnJ0ZgxY/Twww/riSeekI/PuWuh//nPfzRw4EBNnjxZwcHB2rVrl/7yl79o06ZNWrt2rdsxzp49q+HDh2v8+PF66KGHtHDhQk2dOlUFBQV6++239cgjj6hZs2Z6/vnnlZKSog4dOqhz584e/C7XIVYN5nQ6LUmW0+n09igAAKCOS01NtSRZv//9793WMzIyLEnWG2+8YVmWZbVo0cLy9fW1du/e7bZv5MiRlt1ut7KystzWBwwYYAUFBVnHjx+3LMuyHnroIctms1k7duxw29evXz9LkpWZmela69mzpyXJ+vDDDy84e1lZmXX27Flr/fr1liTryy+/dL129913W5Kst99+27V29uxZq1GjRpYk64svvnCt5+fnW76+vtaUKVMueL7z1fde4zYPAACA89xxxx1uz0eMGCE/Pz9lZma61q655hq1adPGbd/atWuVnJysuLg4t/WUlBSdPHlSn376qSRp/fr16tChg9q3b++2b9SoURXOEx4erl69epVb37dvn0aPHq3Y2Fj5+vqqQYMG6tmzpyTpm2++cdtrs9k0cOBA13M/Pz+1bt1ajRs31rXXXutaj4iIUHR0tL777rsKZ0F53OYBAABwntjYWLfnfn5+ioyMVH5+vmutcePG5b4uPz+/wvUmTZq4Xv/p/8bHx5fbFxMTU+E8FR3zxIkT6tGjhwICAvTnP/9Zbdq0UVBQkA4ePKjhw4eX+4XIoKAgBQQEuK35+/srIiKi3LH9/f11+vTpCmdBecQ0AACol6zSUp3c8rlKDh+WX6NGssrKJEm5ublq2rSpa19JSYny8/MVGRnpWrPZbOWOFxkZqZycnHLr2dnZkqSoqCjXvry8vHL7cnNzK5yzonOtXbtW2dnZWrdunetqtCQdP368wmPAc4hpAABQ7xSsWqW8J9JVcl7AHj197mpuRkaG2y/fvfnmmyopKVFiYuIFj5mcnKylS5cqOzvbdTVakl577TUFBQXpxhtvlCT17NlTTz/9tHbu3Ol2q8fixYsvef6fAttut7utv/TSS5d8DFwexDQAAKhXClat0g8PTpYsy2297MS5j5R7e+FC+fn5qU+fPq5P8+jYsaNGjBhxweOmpqZqxYoVSkpK0vTp0xUREaGMjAytXLlSTz75pBwOhyRp8uTJevXVVzVgwADNnDlTMTExWrhwoXbt2iVJrk/ruJCEhASFh4dr/PjxSk1NVYMGDZSRkaEvv/zS4DuCX4JfQAQAAPWGVVqqvCfSy4X0j69KkmY3baZd33yj4cOHa/r06Ro8eLBWrVolf3//Cx67bdu2+uSTT9S2bVtNmDBBQ4cO1fbt2zVv3jw99NBDrn1NmjTR+vXr1aZNG40fP1533HGH/P39NXPmTElSw4YNL/o+IiMjtXLlSgUFBWnMmDH67W9/q5CQEC1ZsuSSvxe4PGyWVeF/m2qEgoICORwOOZ1OhYWFeXscAABQyxV9tklZd99d4WsvHDmsOfn5+t9WrdUxI0PBXW+o1tnGjRunRYsWKT8//6LhXpPU917jNg8AAFBvlJz3lwwvxz5TM2fOVJMmTfSrX/1KJ06c0IoVK/SPf/xDjz32WK0KaRDTAACgHvFr1Oiy7jPVoEEDPfXUU/r+++9VUlKiK664Qs8884wefPBBj54Xlx+3eQAAgHrDKi3VnuTeKsnLq/i+aZtNfjExav3hGtl8fat/wFqovvcav4AIAADqDZuvr2KmTf3xyc8+v/nH5zHTphLSuGTENAAAqFfC+vZV09nPyu9nf3HQLyZGTWc/q7C+fb00GWoj7pkGAAD1TljfvgpNTnb7C4hB13fmijSqjJgGAAD1ks3Xt9o//g51D7d5AAAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwFC1xPScOXMUHx+vgIAAde7cWR999FF1nBYAAADwKI/H9JIlSzR58mQ9+uij2rp1q3r06KEBAwYoKyvL06cGAAAAPMpmWZblyRN07dpV1113nebOnetaa9eunYYOHar09PQLfm1BQYEcDoecTqfCwsI8OSYAAAAM1Pde8+iV6TNnzujzzz9X37593db79u2rTz75pNz+4uJiFRQUuD0AAACAmsqjMX3kyBGVlpYqJibGbT0mJka5ubnl9qenp8vhcLgecXFxnhwPAAAA+EWq5RcQbTab23PLssqtSdLUqVPldDpdj4MHD1bHeAAAAIARP08ePCoqSr6+vuWuQh86dKjc1WpJstvtstvtnhwJAAAAuGw8emXa399fnTt31urVq93WV69erYSEBE+eGgAAAPA4j16ZlqQpU6bozjvv1PXXX69u3brp5ZdfVlZWlsaPH+/pUwMAAAAe5fGYvv3225Wfn6+ZM2cqJydHHTp00L/+9S+1aNHC06cGAAAAPMrjnzP9S9T3zy0EAACo6ep7r1XLp3kAAAAAdRExDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAtdacOXM0f/58r52fmAYAAECtRUwDAAAANYhlWTp16tQl7SWmAQAA6rhdu3Zp1KhRiomJkd1uV/PmzXXXXXepuLhYkrR9+3YNGTJE4eHhCggIUKdOnbRgwQK3Y6xbt042m00LFy7UI488osaNGyskJES33367JKmwsFDjxo1TVFSUoqKiNHbsWJ04ccLtGDabTRMnTtRLL72kNm3ayG63q3379lq8eLHbvrS0NNlstnLvY/78+bLZbDpw4IAkqWXLltqxY4fWr18vm80mm82mli1buvYXFBToD3/4g+Lj4+Xv76+mTZtq8uTJKioqqnCuF198Ue3atZPdbi/3/ivjd0m7AAAAUCt9+eWX6t69u6KiojRz5kxdccUVysnJ0fLly3XmzBkdOHBACQkJio6O1nPPPafIyEi98cYbSklJUV5enh5++GG3402bNk1JSUmaP3++Dhw4oD/84Q+SpDvvvFOdO3fWokWLtHXrVk2bNk2hoaF67rnn3L5++fLlyszM1MyZMxUcHKw5c+Zo1KhR8vPz029+85sqvbelS5fqN7/5jRwOh+bMmSNJstvtkqSTJ0+qZ8+e+v777zVt2jRdc8012rFjh6ZPn66vv/5aa9ascQv2ZcuW6aOPPtL06dMVGxur6OjoSxvCqsGcTqclyXI6nd4eBQAAoFbq1auX1bBhQ+vQoUMVvj5y5EjLbrdbWVlZbusDBgywgoKCrOPHj1uWZVmZmZmWJGvw4MFu+x544AFLkjV+/Hi39aFDh1oRERFua5KswMBAKzc317VWUlJiXXnllVbr1q1da6mpqVZFmTpv3jxLkrV//37X2lVXXWX17Nmz3N709HTLx8fH2rx5s9v6W2+9ZUmy/vWvf7nN5XA4rKNHj5Y7zsVwmwcAAEAddfLkSa1fv14jRoxQo0aNKtyzdu1aJScnKy4uzm09JSVFJ0+e1Keffuq2PmjQILfnbdq0kST17dvXbb1du3Y6evRouVs9kpOTFRMT43ru6+ur22+/XXv27NH3339ftTd4AStWrFCHDh3UqVMnlZSUuB79+vWTzWbTunXr3Pb36tVL4eHhVT4PMQ0AAFBHHTt2TKWlpWrWrFmle/Lz89W4ceNy602aNHG9fr6IiAi35/7+/pJULkR/Wj99+rTbemxsbLlz/bT283P9Enl5efrqq6/UoEEDt0doaKgsy9KRI0fc9lf0PbgU3DMNAABQV5SVSt99Ip3Ik0JiFBF9rXx9fS94xTcyMlI5OTnl1rOzsyVJUVFRl3XE3NzcStciIyMlSQEBAZKk4uJi1z3QksoF8IVERUUpMDBQr776aqWvn6+iX3i8FMQ0AABAXbBzufTBI1JBtmspMKyJel5/lf75z3/q8ccfrzCMk5OTtXTpUmVnZ7uuRkvSa6+9pqCgIN14442XdcwPP/xQeXl5rls9SktLtWTJErVq1cp1Bf2nT+T46quv1KVLF9fXvvfee+WOZ7fbK/wYu0GDBumJJ55QZGSk4uPjL+t7OB+3eQAAANR2O5dLb97lFtKSpIIcPdNpn84Wn1LXrl3197//XZmZmVq8eLFGjx6twsJCpaamqkGDBkpKSlJGRobef/99jRkzRitXrlRaWpocDsdlHTUqKkq9evXS4sWL9d5772nQoEHatWuXHn/8cdeegQMHKiIiQvfcc4+WLVumFStW6De/+Y0OHjxY7nhXX321vvzySy1ZskSbN2/W119/LUmaPHmy2rZtq5tuuknPPPOM1qxZo1WrVukf//iHRowYoc8+++yyvB+uTAMAANRmZaXnrkjLquBFSx1jfbVpYrRS912nqVOnqrCwULGxserVq5f8/f3Vtm1bffLJJ5o2bZomTJigU6dOqV27dpo3b55SUlIu+7i33HKLrrrqKj322GPKyspSq1atlJGR4fq8akkKCwvTBx98oMmTJ2vMmDFq2LCh7r33Xg0YMED33nuv2/FmzJihnJwc3XfffSosLFSLFi104MABBQcH66OPPtKsWbP08ssva//+/QoMDFTz5s3Vu3dvt8+j/iVsP34cSI1UUFAgh8Mhp9OpsLAwb48DAABQ8+z/SFow6OL77l4hxfe47KevSq/ZbDZNmDBBL7zwwmWfw1u4zQMAAKA2O5F3efehSohpAACA2iwk5uJ7qrIPVcI90wAAALVZiwQprIlUkKOK75u2nXu9RUJ1T1ZODb672BhXpgEAAGozH1+p/19+fPLzz0r+8Xn/Wef24bIjpgEAAGq79rdII16Twn72V/zCmpxbb3+Ld+aqB7jNAwAAoC5of4t05c1ufwFRLRK4Iu1hxDQAAEBd4ePrkY+/Q+W4zQMAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIY/G9OOPP66EhAQFBQWpYcOGnjwVAAAAUO08GtNnzpzRbbfdpvvvv9+TpwEAAAC8ws+TB58xY4Ykaf78+Z48DQAAAOAVHo3pqiouLlZxcbHreUFBgRenAQAAAC6sRv0CYnp6uhwOh+sRFxfn7ZEAAACASlU5ptPS0mSz2S742LJli9EwU6dOldPpdD0OHjxodBwAAACgOlT5No+JEydq5MiRF9zTsmVLo2HsdrvsdrvR1wIAAADVrcoxHRUVpaioKE/MAgAAANQqHv0FxKysLB09elRZWVkqLS3Vtm3bJEmtW7dWSEiIJ08NAAAAeJxHY3r69OlasGCB6/m1114rScrMzFRiYqInTw0AAAB4nM2yLMvbQ1SmoKBADodDTqdTYWFh3h4HAAAAP1Pfe61GfTQeAAAAUJsQ0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaqKJTp055ewQAAFBDENOol9LS0mSz2bR161YNHz5cYWFhcjgcGjNmjA4fPuza17JlSw0aNEjvvPOOrr32WgUEBGjGjBmSpO3bt2vIkCEKDw9XQECAOnXqpAULFpQ71/Hjx/Vf//Vf+tWvfiW73a7o6GgNHDhQu3btcu05c+aM/vznP+vKK6+U3W5Xo0aNNHbsWLdZJGnt2rVKTExUZGSkAgMD1bx5c9166606efKka8/cuXPVsWNHhYSEKDQ0VFdeeaWmTZt2ub+FAABAkp+3BwC8adiwYRoxYoTGjx+vHTt26E9/+pN27typzz77TA0aNJAkffHFF/rmm2/02GOPKT4+XsHBwdq9e7cSEhIUHR2t5557TpGRkXrjjTeUkpKivLw8Pfzww5KkwsJCde/eXQcOHNAjjzyirl276sSJE9qwYYNycnJ05ZVXqqysTEOGDNFHH32khx9+WAkJCfruu++UmpqqxMREbdmyRYGBgTpw4IBuvvlm9ejRQ6+++qoaNmyoH374QR988IHOnDmjoKAgLV68WA888IAmTZqkp59+Wj4+PtqzZ4927tzpzW8zAAB1l1WDOZ1OS5LldDq9PQrqmNTUVEuS9fvf/95tPSMjw5JkvfHGG5ZlWVaLFi0sX19fa/fu3W77Ro4cadntdisrK8ttfcCAAVZQUJB1/Phxy7Isa+bMmZYka/Xq1ZXOsmjRIkuS9fbbb7utb9682ZJkzZkzx7Isy3rrrbcsSda2bdsqPdbEiROthg0bXuTdAwBw+dT3XuM2D9Rrd9xxh9vzESNGyM/PT5mZma61a665Rm3atHHbt3btWiUnJysuLs5tPSUlRSdPntSnn34qSXr//ffVpk0b9e7du9IZVqxYoYYNG2rw4MEqKSlxPTp16qTY2FitW7dOktSpUyf5+/tr3LhxWrBggfbt21fuWDfccIOOHz+uUaNG6d1339WRI0eq9P0AAABVQ0yjzists/Tp3ny9u+0Hfbo3X6Vlluu12NhYt71+fn6KjIxUfn6+a61x48bljpmfn1/hepMmTVyvS9Lhw4fVrFmzC86Xl5en48ePy9/fXw0aNHB75ObmuoK4VatWWrNmjaKjozVhwgS1atVKrVq10uzZs13HuvPOO/Xqq6/qu+++06233qro6Gh17dpVq1evvti3CQAAGOCeadRpH2zP0Yz3dirHedq11tgRoFaHCiVJubm5atq0qeu1kpIS5efnKzIy0rVms9nKHTcyMlI5OTnl1rOzsyVJUVFRkqRGjRrp+++/v+CMUVFRioyM1AcffFDh66Ghoa7/3KNHD/Xo0UOlpaXasmWLnn/+eU2ePFkxMTEaOXKkJGns2LEaO3asioqKtGHDBqWmpmrQoEH69ttv1aJFiwvOAgAAqoYr06izPtieo/vf+MItpCUp13laK746F8IZGRlur7355psqKSlRYmLiBY+dnJystWvXuuL5J6+99pqCgoJ04403SpIGDBigb7/9VmvXrq30WIMGDVJ+fr5KS0t1/fXXl3u0bdu23Nf4+vqqa9eu+tvf/ibp3C9J/lxwcLAGDBigRx99VGfOnNGOHTsu+J4AAEDVcWUadVJpmaUZ7+2UVcFr56+988478vPzU58+fVyf5tGxY0eNGDHigsdPTU3VihUrlJSUpOnTpysiIkIZGRlauXKlnnzySTkcDknS5MmTtWTJEg0ZMkR//OMfdcMNN+jUqVNav369Bg0apKSkJI0cOVIZGRkaOHCgHnzwQd1www1q0KCBvv/+e2VmZmrIkCEaNmyYXnzxRa1du1Y333yzmjdvrtOnT+vVV1+VJNc92ffdd58CAwP161//Wo0bN1Zubq7S09PlcDjUpUuXy/GtBQAA5yGmUSdt2n+03BXpiqTNflXvvPKs5s6dK5vNpsGDB+vZZ5+Vv7//Bb+ubdu2+uSTTzRt2jRNmDBBp06dUrt27TRv3jylpKS49oWGhurjjz9WWlqaXn75Zc2YMUPh4eHq0qWLxo0bJ+ncVebly5dr9uzZev3115Weni4/Pz81a9ZMPXv21NVXXy3p3C8grlq1SqmpqcrNzVVISIg6dOig5cuXq2/fvpLO3QYyf/58vfnmmzp27JiioqLUvXt3vfbaa2rUqJHhdxMAAFTGZllWRRfvaoSCggI5HA45nU6FhYV5exzUIu9u+0EPLt5W6evHP86Q838X6bW1X+nOpKurbzAAAOqY+t5r3DONOik6NOCS9kWF2D08CQAAqMuIadRJN8RHqLEjQOU/h8PddS3Cq2UeAABQNxHTqJN8fWxKHdxeksoFtU1SePc79P7X2YqJ5j5iAABgjphGndW/Q2PNHXOdYh3ut3zEOgI0d8x16t+h/B9dAQAAqAo+zQN1Wv8OjdWnfaw27T+qQ4WnFR0aoBviI+Trc7EbQAAAAC6OmEad5+tjU7dWkRffCAAAUEXc5gEAAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGPBbTBw4c0D333KP4+HgFBgaqVatWSk1N1ZkzZzx1SgAAAKBa+XnqwLt27VJZWZleeukltW7dWtu3b9d9992noqIiPf300546LQAAAFBtbJZlWdV1sqeeekpz587Vvn37Lml/QUGBHA6HnE6nwsLCPDwdAAAAqqq+95rHrkxXxOl0KiIiotLXi4uLVVxc7HpeUFBQHWMBAAAARqrtFxD37t2r559/XuPHj690T3p6uhwOh+sRFxdXXeMBAAAAVVblmE5LS5PNZrvgY8uWLW5fk52drf79++u2227TvffeW+mxp06dKqfT6XocPHiw6u8IAAAAqCZVvmf6yJEjOnLkyAX3tGzZUgEBAZLOhXRSUpK6du2q+fPny8fn0vu9vt+DAwAAUNPV916r8j3TUVFRioqKuqS9P/zwg5KSktS5c2fNmzevSiENAAAA1HQe+wXE7OxsJSYmqnnz5nr66ad1+PBh12uxsbGeOi0AAABQbTwW06tWrdKePXu0Z88eNWvWzO21avw0PgAAAMBjPHbfRUpKiizLqvABAAAA1AXcxAwAAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAwR0wAAAIAhYhoAAAAwREwDAAAAhohpAAAAwBAxDQAAABgipgEAAABDxDQAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIauAQ7duyQzWbTP//5T9fa559/LpvNpquuuspt7y233KLOnTu7ni9ZskTdunVTcHCwQkJC1K9fP23durXaZgcAAJ5DTAOX4KqrrlLjxo21Zs0a19qaNWsUGBionTt3Kjs7W5JUUlKi9evXq3fv3pKkJ554QqNGjVL79u315ptv6vXXX1dhYaF69OihnTt3euW9AACAy4eYBi5RcnJyuZgeM2aMwsPDXeubNm1SQUGBevfurYMHDyo1NVUTJ07UK6+8optvvlnDhg3TqlWrFBoaqhkzZnjrrQAAgMuEmAYuUXJysvbt26f9+/fr9OnT+vjjj9W/f38lJSVp9erVks4Ftt1uV/fu3fXvf/9bJSUluuuuu1RSUuJ6BAQEqGfPnlq3bp133xAAAPjF/Lw9AFBTlZWV6odvdujE8WMKaRiuXr2SJJ0L5vj4eJ09e1a9evVSXl6e/vu//9v12q9//WsFBgYqLy9PktSlS5cKj+/jwz/LAgBQ2xHTQAX+89knWjv/ZZ04esS1FhIRpfjmcVqzZo1atmyp66+/Xg0bNlRycrIeeOABffbZZ9q4caPr9o2oqChJ0ltvvaUWLVp45X0AAADPIqaBn/nPZ59o+TNPlFs/cfSIGjfw0epVq9QyPl4333yzJKlNmzZq3ry5pk+frrNnz7p++bBfv37y8/PT3r17deutt1brewAAANWDmAbOU1ZWqrXzX6709StiIvXJ3u90bOtWPfvss6715ORkzZs3T+Hh4a6PxWvZsqVmzpypRx99VPv27VP//v0VHh6uvLw8bdq0ScHBwfwSIgAAtRw3bQLn+eGbHW63dvxc6+go2WxSUGCgunXr5lr/6Wp0UlKS273QU6dO1VtvvaVvv/1Wd999t/r166eHH35Y3333nW666SbPvREAAFAtbJZlWd4eojIFBQVyOBxyOp0KCwvz9jioB7753/X613NPXXTfwP/3kNr9umc1TAQAQM1W33uNK9PAeUIahl/WfQAAoG4jpoHzNG13lUIioi64JzQySk3bXXXBPQAAoH4gpoHz+Pj4qlfKuAvuSbp7nHx8fKtpIgAAUJMR08DPXNE1QbdMmVbuCnVoZJRumTJNV3RN8NJkAACgpuGj8YAKXNE1Qa26dHX7C4hN213FFWkAAOCGmAYq4ePjq7irrvH2GAAAoAbjNg8AAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAIWIaAAAAMERMAwAAAIaIaQAAAMAQMQ0AAAAYIqYBAAAAQ8Q0AAAAYIiYBgAAAAz5eXuAC7EsS5JUUFDg5UkAAABQkZ867aduq29qdEwXFhZKkuLi4rw8CQAAAC6ksLBQDofD22NUO5tVg/8xoqysTNnZ2QoNDZXNZvP2OLVKQUGB4uLidPDgQYWFhXl7HFSAn1HtwM+p5uNnVPPxM6odTH9OlmWpsLBQTZo0kY9P/buDuEZfmfbx8VGzZs28PUatFhYWxv9w1XD8jGoHfk41Hz+jmo+fUe1g8nOqj1ekf1L//vEBAAAAuEyIaQAAAMAQMV1H2e12paamym63e3sUVIKfUe3Az6nm42dU8/Ezqh34OZmp0b+ACAAAANRkXJkGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDFdxx04cED33HOP4uPjFRgYqFatWik1NVVnzpzx9mg4z+OPP66EhAQFBQWpYcOG3h4HP5ozZ47i4+MVEBCgzp0766OPPvL2SDjPhg0bNHjwYDVp0kQ2m03Lli3z9kj4mfT0dHXp0kWhoaGKjo7W0KFDtXv3bm+PhfPMnTtX11xzjeuvHnbr1k3vv/++t8eqVYjpOm7Xrl0qKyvTSy+9pB07duivf/2rXnzxRU2bNs3bo+E8Z86c0W233ab777/f26PgR0uWLNHkyZP16KOPauvWrerRo4cGDBigrKwsb4+GHxUVFaljx4564YUXvD0KKrF+/XpNmDBBGzdu1OrVq1VSUqK+ffuqqKjI26PhR82aNdOsWbO0ZcsWbdmyRb169dKQIUO0Y8cOb49Wa/A50/XQU089pblz52rfvn3eHgU/M3/+fE2ePFnHjx/39ij1XteuXXXddddp7ty5rrV27dpp6NChSk9P9+JkqIjNZtPSpUs1dOhQb4+CCzh8+LCio6O1fv163XTTTd4eB5WIiIjQU089pXvuucfbo9QKXJmuh5xOpyIiIrw9BlBjnTlzRp9//rn69u3rtt63b1998sknXpoKqP2cTqck8f+DaqjS0lItXrxYRUVF6tatm7fHqTX8vD0AqtfevXv1/PPP63/+53+8PQpQYx05ckSlpaWKiYlxW4+JiVFubq6XpgJqN8uyNGXKFHXv3l0dOnTw9jg4z9dff61u3brp9OnTCgkJ0dKlS9W+fXtvj1VrcGW6lkpLS5PNZrvgY8uWLW5fk52drf79++u2227Tvffe66XJ6w+TnxFqFpvN5vbcsqxyawAuzcSJE/XVV19p0aJF3h4FP9O2bVtt27ZNGzdu1P3336+7775bO3fu9PZYtQZXpmupiRMnauTIkRfc07JlS9d/zs7OVlJSkrp166aXX37Zw9NBqvrPCDVHVFSUfH19y12FPnToULmr1QAubtKkSVq+fLk2bNigZs2aeXsc/Iy/v79at24tSbr++uu1efNmzZ49Wy+99JKXJ6sdiOlaKioqSlFRUZe094cfflBSUpI6d+6sefPmyceHfyFRHaryM0LN4u/vr86dO2v16tUaNmyYa3316tUaMmSIFycDahfLsjRp0iQtXbpU69atU3x8vLdHwiWwLEvFxcXeHqPWIKbruOzsbCUmJqp58+Z6+umndfjwYddrsbGxXpwM58vKytLRo0eVlZWl0tJSbdu2TZLUunVrhYSEeHe4emrKlCm68847df3117v+jU5WVpbGjx/v7dHwoxMnTmjPnj2u5/v379e2bdsUERGh5s2be3Ey/GTChAlauHCh3n33XYWGhrr+bY/D4VBgYKCXp4MkTZs2TQMGDFBcXJwKCwu1ePFirVu3Th988IG3R6s1+Gi8Om7+/PkaO3Zsha/xo685UlJStGDBgnLrmZmZSkxMrP6BIOncH2158sknlZOTow4dOuivf/0rH+dVg6xbt05JSUnl1u+++27Nnz+/+gdCOZX9jsG8efOUkpJSvcOgQvfcc48+/PBD5eTkyOFw6JprrtEjjzyiPn36eHu0WoOYBgAAAAxx8ywAAABgiJgGAAAADBHTAAAAgCFiGgAAADBETAMAAACGiGkAAADAEDENAAAAGCKmAQAAAEPENAAAAGCImAYAAAAMEdMAAACAof8PWNWHjzA6UJIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(list(embedded_words.values()))\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in enumerate(sample_words):\n",
    "    x, y = embeddings_2d[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4112e1",
   "metadata": {},
   "source": [
    "CBOW on Vietnamese dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc685cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['danh_sách', 'tác_phẩm', 'doraemon'],\n",
      " ['doraemon',\n",
      "  'nguyên_tác',\n",
      "  'là',\n",
      "  'xê',\n",
      "  'ri',\n",
      "  'anime',\n",
      "  'và',\n",
      "  'manga',\n",
      "  'được',\n",
      "  'sáng_tác',\n",
      "  'bởi',\n",
      "  'fujiko_f'],\n",
      " ['bắt_đầu_từ',\n",
      "  'những',\n",
      "  'chương',\n",
      "  'truyện',\n",
      "  'nhỏ',\n",
      "  'đăng',\n",
      "  'trên',\n",
      "  'tạp_chí',\n",
      "  'tháng',\n",
      "  'corocoro',\n",
      "  'comic',\n",
      "  'sau',\n",
      "  'đó',\n",
      "  'shogakukan',\n",
      "  'tập_hợp',\n",
      "  'lại',\n",
      "  'phát_hành',\n",
      "  'theo',\n",
      "  'cuốn',\n",
      "  'từ',\n",
      "  'tập',\n",
      "  '1',\n",
      "  'đến',\n",
      "  'tập',\n",
      "  '45'],\n",
      " ['sau',\n",
      "  'đó',\n",
      "  'chuyển_thể',\n",
      "  'thành',\n",
      "  'anime',\n",
      "  'theo',\n",
      "  'ba',\n",
      "  'giai_đoạn',\n",
      "  'phim',\n",
      "  'hoạt_hình',\n",
      "  '1973',\n",
      "  '1979',\n",
      "  '2005',\n",
      "  'và',\n",
      "  '2005'],\n",
      " ['năm',\n",
      "  '2014',\n",
      "  'viz_media',\n",
      "  'mua',\n",
      "  'bản_quyền',\n",
      "  'mỹ_hóa',\n",
      "  'các',\n",
      "  'yếu_tố',\n",
      "  'trong',\n",
      "  'phim',\n",
      "  'như',\n",
      "  'tên',\n",
      "  'nhân_vật',\n",
      "  'địa_điểm']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "with open('datatrain.txt', 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "dataset = [tokenizer.tokenize(line.lower()) for line in lines if line]\n",
    "pprint(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879cef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 191044\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "vocab = set()\n",
    "for sentence in dataset:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "print(f'Vocab size: {len(vocab)}')\n",
    "word2ind = {w:i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba6785e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['doraemon', 'nguyên_tác', 'xê', 'ri'], 'là'),\n",
       " (['nguyên_tác', 'là', 'ri', 'anime'], 'xê'),\n",
       " (['là', 'xê', 'anime', 'và'], 'ri'),\n",
       " (['xê', 'ri', 'và', 'manga'], 'anime'),\n",
       " (['ri', 'anime', 'manga', 'được'], 'và')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build context and target pairs\n",
    "context_size = 2 \n",
    "datatrain = []\n",
    "for sentence in dataset:\n",
    "    for token in sentence[context_size:-context_size]:  # ignore boundary tokens\n",
    "        token_index = sentence.index(token)\n",
    "        context = sentence[token_index - context_size: token_index] + sentence[token_index + 1: token_index + context_size + 1]\n",
    "        target = token\n",
    "        datatrain.append((context, target))\n",
    "datatrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4a58ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['doraemon', 'nguyên_tác', 'xê', 'ri'], 'là')\n",
      "tensor([157270,  33164, 175476,  85036]) 177843\n"
     ]
    }
   ],
   "source": [
    "def make_context_vector(context, word2ind):\n",
    "    idxs = [word2ind[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "print(datatrain[0])\n",
    "print(make_context_vector(datatrain[0][0], word2ind), word2ind[datatrain[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a71b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "embedding_dim = 300\n",
    "cbow = CBOW(vocab_size=len(vocab), embedding_dim=embedding_dim)\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a9d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b700113",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m cbow\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(nll_prob, target_vector)\n\u001b[1;32m---> 17\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\cs224\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\cs224\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\cs224\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loops\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cbow.to(device)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in datatrain:\n",
    "        context_vector = make_context_vector(context, word2ind).to(device)\n",
    "        target_vector = torch.tensor([word2ind[target]], dtype=torch.long).to(device)\n",
    "\n",
    "        nll_prob = cbow(context_vector)\n",
    "\n",
    "        cbow.zero_grad()\n",
    "\n",
    "        l = loss(nll_prob, target_vector)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += l.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "    losses.append(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
