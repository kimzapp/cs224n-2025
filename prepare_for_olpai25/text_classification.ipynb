{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b14f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
    "train_ds = ds['train']\n",
    "val_ds = ds['validation']\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44c1cd",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101a5e5",
   "metadata": {},
   "source": [
    "### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16575e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       705\n",
      "           1       0.56      0.21      0.30        73\n",
      "           2       0.91      0.93      0.92       805\n",
      "\n",
      "    accuracy                           0.90      1583\n",
      "   macro avg       0.79      0.69      0.71      1583\n",
      "weighted avg       0.89      0.90      0.89      1583\n",
      "\n",
      "Test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90      1409\n",
      "           1       0.59      0.16      0.25       167\n",
      "           2       0.91      0.91      0.91      1590\n",
      "\n",
      "    accuracy                           0.89      3166\n",
      "   macro avg       0.79      0.67      0.69      3166\n",
      "weighted avg       0.88      0.89      0.87      3166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(train_ds['sentence'])\n",
    "\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(X_train_tfidf, train_ds['sentiment'])\n",
    "\n",
    "X_val_tfidf = tfidf.transform(val_ds['sentence'])\n",
    "val_preds = xgboost.predict(X_val_tfidf)\n",
    "\n",
    "print('Validation set:')\n",
    "print(classification_report(val_ds['sentiment'], val_preds))\n",
    "\n",
    "X_test_tfidf = tfidf.transform(test_ds['sentence'])\n",
    "test_preds = xgboost.predict(X_test_tfidf)\n",
    "print(\"Test set:\")\n",
    "print(classification_report(test_ds['sentiment'], test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c32c16",
   "metadata": {},
   "source": [
    "### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "batch = [utils.simple_preprocess(doc) for doc in train_ds['sentence']]\n",
    "\n",
    "# train word2vec\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences=batch, vector_size=100, window=5, min_count=2, workers=4, epochs=50)\n",
    "\n",
    "w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f35455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def w2v_transform(sentences, w2v_model):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        word_embeddings = [w2v_model.wv[word] if word in w2v_model.wv else np.zeros(w2v_model.vector_size) for word in sent]\n",
    "        if word_embeddings:\n",
    "            sent_vector = sum(word_embeddings) / len(word_embeddings)\n",
    "        else:\n",
    "            sent_vector = np.zeros(w2v_model.vector_size)\n",
    "        vectors.append(sent_vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa4474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set with Word2Vec:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.90       705\n",
      "           1       0.71      0.16      0.27        73\n",
      "           2       0.91      0.91      0.91       805\n",
      "\n",
      "    accuracy                           0.89      1583\n",
      "   macro avg       0.83      0.67      0.69      1583\n",
      "weighted avg       0.88      0.89      0.87      1583\n",
      "\n",
      "Test set with Word2Vec:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      1409\n",
      "           1       0.49      0.11      0.18       167\n",
      "           2       0.91      0.89      0.90      1590\n",
      "\n",
      "    accuracy                           0.87      3166\n",
      "   macro avg       0.74      0.64      0.65      3166\n",
      "weighted avg       0.85      0.87      0.85      3166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_sentences = [utils.simple_preprocess(doc) for doc in train_ds['sentence']]\n",
    "train_vectorized = w2v_transform(train_sentences, w2v)\n",
    "\n",
    "xgboost_w2v = XGBClassifier()\n",
    "xgboost_w2v.fit(train_vectorized, train_ds['sentiment'])\n",
    "\n",
    "val_sentences = [utils.simple_preprocess(doc) for doc in val_ds['sentence']]\n",
    "val_vectorized = w2v_transform(val_sentences, w2v)\n",
    "val_preds_w2v = xgboost_w2v.predict(val_vectorized)\n",
    "print('Validation set with Word2Vec:')\n",
    "print(classification_report(val_ds['sentiment'], val_preds_w2v))\n",
    "\n",
    "test_sentences = [utils.simple_preprocess(doc) for doc in test_ds['sentence']]\n",
    "test_vectorized = w2v_transform(test_sentences, w2v)\n",
    "test_preds_w2v = xgboost_w2v.predict(test_vectorized)\n",
    "print(\"Test set with Word2Vec:\")\n",
    "print(classification_report(test_ds['sentiment'], test_preds_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56315edf",
   "metadata": {},
   "source": [
    "### FastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ece167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       705\n",
      "           1       0.59      0.14      0.22        73\n",
      "           2       0.92      0.91      0.91       805\n",
      "\n",
      "    accuracy                           0.88      1583\n",
      "   macro avg       0.79      0.66      0.68      1583\n",
      "weighted avg       0.87      0.88      0.87      1583\n",
      "\n",
      "Test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87      1409\n",
      "           1       0.56      0.09      0.15       167\n",
      "           2       0.89      0.89      0.89      1590\n",
      "\n",
      "    accuracy                           0.86      3166\n",
      "   macro avg       0.76      0.63      0.64      3166\n",
      "weighted avg       0.85      0.86      0.84      3166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "train_sentences = [utils.simple_preprocess(doc) for doc in train_ds['sentence']]\n",
    "fasttext = FastText(sentences=train_sentences, vector_size=128, window=5, min_count=2, workers=4, epochs=50)\n",
    "fasttext.save(\"fasttext.model\")\n",
    "\n",
    "def fasttext_transform(sentences, ft_model):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        word_embeddings = [ft_model.wv[word] if word in ft_model.wv else np.zeros(ft_model.vector_size) for word in sent]\n",
    "        if word_embeddings:\n",
    "            sent_vector = sum(word_embeddings) / len(word_embeddings)\n",
    "        else:\n",
    "            sent_vector = np.zeros(ft_model.vector_size)\n",
    "        vectors.append(sent_vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "train_vectorized_ft = fasttext_transform(train_sentences, fasttext)\n",
    "xgboost_ft = XGBClassifier()\n",
    "xgboost_ft.fit(train_vectorized_ft, train_ds['sentiment'])\n",
    "\n",
    "val_sentences = [utils.simple_preprocess(doc) for doc in val_ds['sentence']]\n",
    "val_vectorized_ft = fasttext_transform(val_sentences, fasttext)\n",
    "val_preds = xgboost_ft.predict(val_vectorized_ft)\n",
    "print('Validation set:')\n",
    "print(classification_report(val_ds['sentiment'], val_preds))\n",
    "\n",
    "test_sentences = [utils.simple_preprocess(doc) for doc in test_ds['sentence']]\n",
    "test_vectorized_ft = fasttext_transform(test_sentences, fasttext)\n",
    "test_preds = xgboost_ft.predict(test_vectorized_ft)\n",
    "print(\"Test set:\")\n",
    "print(classification_report(test_ds['sentiment'], test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbe9ea",
   "metadata": {},
   "source": [
    "### LSTM/GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01a988ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models import FastText\n",
    "\n",
    "train_sentences = [utils.simple_preprocess(doc) for doc in train_ds['sentence']]\n",
    "val_sentences = [utils.simple_preprocess(doc) for doc in val_ds['sentence']]\n",
    "test_sentences = [utils.simple_preprocess(doc) for doc in test_ds['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8aeddc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import torch\n",
    "\n",
    "words = []\n",
    "for sent in train_sentences:\n",
    "    words.extend(sent)\n",
    "words = list(set(sorted(words)))\n",
    "word2ind = {w:idx+2 for idx, w in enumerate(words)} # index 0 for pad token, index 0 for unknown token\n",
    "ind2word = {idx+2:w for idx, w in enumerate(words)}\n",
    "\n",
    "pad_token = '<PAD>'\n",
    "pad_token_id = 0\n",
    "word2ind[pad_token] = pad_token_id\n",
    "ind2word[pad_token_id] = pad_token\n",
    "\n",
    "unk_token = '<UNK>'\n",
    "unk_token_id = 1\n",
    "word2ind[unk_token] = unk_token_id\n",
    "ind2word[unk_token_id] = unk_token\n",
    "\n",
    "vocab_size = len(word2ind.keys())\n",
    "\n",
    "def tokenize(input_sentences, word2ind, unk_token='<UNK>'):\n",
    "    processed_sents = [utils.simple_preprocess(sent) for sent in input_sentences]\n",
    "    sentence_ids = []\n",
    "    unk_token_id = word2ind[unk_token]\n",
    "    for processed_sent in processed_sents:\n",
    "        ids = [word2ind.get(word, unk_token_id) for word in processed_sent]\n",
    "        sentence_ids.append(ids)\n",
    "    return sentence_ids\n",
    "\n",
    "def pad_sequence(input_sequences, word2ind, pad_token):\n",
    "    max_len = max([len(seq) for seq in input_sequences])\n",
    "    pad_token_id = word2ind[pad_token]\n",
    "    padded_sequences = [seq + [pad_token_id]*(max_len - len(seq)) for seq in input_sequences]\n",
    "    return padded_sequences\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences = [sample['sentence'] for sample in batch]\n",
    "    labels = [sample['sentiment'] for sample in batch]\n",
    "    tokenized_sents = tokenize(input_sentences=sentences, word2ind=word2ind, unk_token=unk_token)\n",
    "    padded_sequences = pad_sequence(input_sequences=tokenized_sents, word2ind=word2ind, pad_token=pad_token)\n",
    "    return torch.tensor(padded_sequences, dtype=torch.int32), torch.tensor(labels, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88235f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized seq:\n",
      "[[1126, 388, 1028, 2125, 1847], [1672, 1087, 810, 1815, 409, 655, 1516, 306, 643], [1320, 966, 2125, 1847, 2123, 545, 843, 1246], [1323, 2144, 610, 1946, 575, 1093, 1624, 1782, 523, 693, 109, 457, 246, 1908, 1338, 810, 1815]]\n",
      "Padded seq:\n",
      "[[1126, 388, 1028, 2125, 1847, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1672, 1087, 810, 1815, 409, 655, 1516, 306, 643, 0, 0, 0, 0, 0, 0, 0, 0], [1320, 966, 2125, 1847, 2123, 545, 843, 1246, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1323, 2144, 610, 1946, 575, 1093, 1624, 1782, 523, 693, 109, 457, 246, 1908, 1338, 810, 1815]]\n"
     ]
    }
   ],
   "source": [
    "samples = train_ds['sentence'][:4]\n",
    "tokenized_sents = tokenize(input_sentences=samples, word2ind=word2ind, unk_token=unk_token)\n",
    "print(\"Tokenized seq:\")\n",
    "print(tokenized_sents)\n",
    "\n",
    "padded_sequences = pad_sequence(input_sequences=tokenized_sents, word2ind=word2ind, pad_token='<PAD>')\n",
    "print('Padded seq:')\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbf635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout_p=0.1, pad_token_id=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n",
    "\n",
    "        # GRU\n",
    "        self.gru = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=(0 if num_layers==1 else dropout_p),\n",
    "            batch_first=True,\n",
    "            bidirectional=True # bidirection để nắm bắt ngữ nghĩa tốt hơn\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout_p\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        embedded = self.dropout(self.embedding(input_seqs))\n",
    "        # embedded: [B, seq_len, embedding_dim]\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout_p=0.1):\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_size, num_layers, dropout_p)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        features = self.encoder(input_seqs)\n",
    "        logits = self.mlp(features)\n",
    "        return torch.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c086bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=4, collate_fn=collate_fn, shuffle=True, num_workers=0, generator=generator)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=4, collate_fn=collate_fn, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f58b7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=vocab_size, embedding_dim=128, padding_idx=0),\n",
    "    nn.LSTM(input_size=128, hidden_size=10, num_layers=2, batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d133516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 57, 10])\n",
      "torch.Size([2, 4, 10])\n",
      "torch.Size([2, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    seqs, labels = batch\n",
    "    outputs, (hiddens, cells) = test(seqs)\n",
    "    print(outputs.shape)\n",
    "    print(hiddens.shape)\n",
    "    print(cells.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = SentimentClassifier(\n",
    "    num_classes=3,\n",
    "    vocab_size=\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
